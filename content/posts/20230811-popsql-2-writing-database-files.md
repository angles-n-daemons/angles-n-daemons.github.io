---
title: "popSQL part 2: writing the sqlite database file"
date: 2023-08-11T10:26:00-04:00
tags: ["sql", "python", "sqlite"]
categories: ["software"]
---

# Writing out what we read

The next part of this exercise is going to be writing back out the database files we read in. Hopefully by the end of the exercise we should be able to run our new main function:

```python
# main.py
...

+ def test_file_end_to_end():
+     # create the pagers for both the old and new dbs
+     old_db_pager = Pager('test.db')
+     new_db_pager = Pager('generated.db')
+ 
+     # read in the schema page
+     schema_page = old_db_pager.get_page(1)
+     dbinfo = DBInfo(schema_page)
+     schema_node = Node(schema_page, True)
+ 
+     # read in the page with the test table
+     data_page = old_db_pager.get_page(2)
+     data_node = Node(data_page)
+ 
+     # serialize the pages
+     new_schema_page = dbinfo.to_bytes() + schema_node.to_bytes(dbinfo)
+     new_data_page = data_node.to_bytes(dbinfo)
+ 
+     # write the pages to the new db
+     new_db_pager.write_page(1, new_schema_page)
+     new_db_pager.write_page(2, new_data_page)

if __name__ == '__main__':
-   test_schema_page()
+   test_file_end_to_end()
```

For this to work we need to introduce some new functions that allow us to serialize our database header and nodes:

```python
# src/dbinfo.py
...
+   def to_bytes(self) -> bytes:
+       return bytes([])
+
    def _debug(self):
```

```python
# src/backend/node.py
...
+from src.dbinfo import DBInfo
from src.util import b2i
...

+   def to_bytes(self, dbinfo: DBInfo) -> bytes:
+       return bytes([])
+
    def _debug(self):
```

As well as a function that allows us to use our pager to write file content:

```python
# src/backend/pager.py
...
+   def write_page(
+       self,
+       page_number: int,
+       data: bytes,
+   ):
+       with open(self.file_name, 'rb+') as file:
+           file.seek(self.get_offset(page_number))
+           file.write(data)

    def get_offset(
...
```

If we haven't touched the `test.db` file since we created it last week, then we should be able to read the file generated by the program in the same way:
```
➜  sqlite3 test.db
SQLite version 3.39.5 2022-10-14 20:58:05
Enter ".help" for usage hints.
sqlite> select * from test;
hi|1
yo|2
```

Our goal for this post is to do the same with a file "generated.db" which is written out by our program.

We can test that our above skeleton works by creating the output file and running our main file once more:

```
➜  touch generated.db
➜  python3 main.py
```

If this works without an error then we can get started serializing the different bits of the database.

# Writing out the Records and Cells

We'll start by rewriting out the record values. If you recall, Records contain a header which include a varint for the header size as well as varints for each of the column types. We'll start with the column types, by creating a function `to_int` which turns `Column` class which we created back into the integer value it came in as.

```python
# src/backend/record.py
...

class Column
    def to_int(self) -> int:
        if self.type.value < 12:
            return self.type.value

        modifier = self.type.value
        return (self.length * 2) + modifier

```

From here we can add a new `to_varint` function which takes our integer values and turns them back into varint byte arrays. A first pass of this yeilds us:

```python
# src/util.py
...
def to_varint(x: int) -> bytes:
    """
    to_varint takes an integer and turns it into a variable length byte array

    it takes x, an integer

    and returns a varint representation as a byte array
    """

    result = bytearray()
    for j in range(8):
        byte = x % 16
        x = x >> 7

        if x > 0:
            # add the carry bit if there's more information
            byte = byte | 0x80

        result.append(byte)

        if x <= 0:
            return bytes(result)

    byte = x % 32
    result.append(byte)
    return bytes(result)
```

However there's an issue, running the following test doesn't work:

```python
# test.py (not saved in git)

from src.util import varint, to_varint

broken_example = bytes([0x81, 0x91, 0xd1, 0xac, 0x78])
recomputed = to_varint(varint(broken_example, 0)[0])
print(broken_example.hex(), recomputed.hex())
assert broken_example == recomputed
```

Running the above test we see a failure:

```
➜  python3 test.py
8191d1ac78 888c818101
Traceback (most recent call last):
  File "/Users/godzilla/projects/pypopsql/test.py", line 6, in <module>
    assert broken_example == recomputed
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError
```

Turns out generating varints is a bit more complicated than I had originally expected. I hinged my assumption on being able to simply:

```
bytes.append(x % 128)
x = x >> 7
```

But this turns out not to be the right approach. For one, the bytes are going to be in the wrong order (smallest left) and for another we're going to be unsure how large the last byte is to be. In this particular case, the last byte we refer to is the last byte read, which will be the first byte written due to how we intend to pull off the bits.

We can find out if 9 bytes will be needed by seeimg if any of the first 7 bits of the long are filled. If any of them are filled (1), we will need a full 9 bytes to express the varint and therefore our first byte written will be 8 bits in size.

This is simple enough to express with an arithmetic or operation: `requires_9_bytes = bool(0xfe00000000000000 & x)`

We can use this to modify our `to_varint` implementation to say whether the first byte that needs to be pulled off is to be the 9th byte of our varint. Then we can insert instead of append the bytes in order until the payload is filled.

```python
# src/util.py

...
def to_varint(x: int) -> bytes:
    """
    to_varint takes an integer and turns it into a variable length byte array

    it takes x, an integer

    and returns a varint representation as a byte array
    """
    result = bytearray()

    first_7_bits = 0xfe00000000000000
    requires_9_bytes = x & first_7_bits

    # if any of the first 7 bits are filled, 9 bytes will be needed
    first_shift_size = 8 if requires_9_bytes else 7
    first_modulo = 256 if requires_9_bytes else 128

    result.append(x % first_modulo)
    x = x >> first_shift_size

    while x > 0:
        # pull first 7 bits, flip the first bit to signal carryover
        byte = (x % 128) | 0x80
        result.insert(0, byte)
        x = x >> 7

    return bytes(result)
```

Then we can add the following line to our utility test to see if the output of `to_varint` is the same as what we read for `varint`:

```
     input_data = bytes(test.data)
     result, cursor = varint(input_data, 0)
     self.assertEqual(test.expected, result)
     self.assertEqual(test.cursor, cursor)
+    self.assertEqual(input_data.hex(), to_varint(result).hex())
```

So no we've put the two things together for generating bytes for the Record header, we'll need to encode the column types as well as the header length. The header length has an interesting little quirk as its own size contributes to its value. This has a unique edge case:

Per the file format documentation, often the header_size is going to be encoded in a single byte. This is because all numbers 127 and below can be encoded in a single byte, and it's unlikely that a record has so many columns that it needs to communicate a length greater than 127.

Consider for a moment the unlikely case that a Record has 127 TINYINT columns. Encoding a varint of every number 127 and below only takes a single byte. If we kept the assumption earlier that we would only need a single byte, we could say the whole header would be 128 bytes long because it would be the length of the columns - in addition to the space required to encode the payload size. However when we try to encode the number 128 it ends up in 2 bytes - making the actual header size 129 even though the number encoded is 128.

For this reason I'll special case the number 127 as the only possible length for column information in which we'd override the modifier. Because the next number where this happens is 32,766; a number extremely unlikely to occur - I'll simply set an error case on numbers that enter that realm:

```python
# src/backend/record.py

...
class Record
...
    def read_value(
...

+   def header_bytes(self) -> bytes:
+       payload = bytearray()
+
+       for column in self.columns:
+           payload += column.to_bytes()
+
+       payload_size = len(payload)
+       if payload_size > 32767:
+           raise Exception(f'unexpected header payload size {payload_size}')
+
+       expected_varint_length = 1 if payload_size < 128 else 2
+       header_size_bytes = to_varint(payload_size + expected_varint_length)
+       return bytes(header_size_bytes + payload)
```

Once we've effectively written out the header payload, we can serialize the body in a similar manner to how we deserialized it originally.

```python
# src/backend/record.py
...
    def header_bytes(self) -> bytes:
...

+   @staticmethod
+   def value_bytes(column: Column, value: any) -> bytes:
+       # Note: column types other than NULL, TINYINT, and TEXT are omitted
+       if column.type == ColumnType.NULL:
+           return bytes([])
+       elif column.type == ColumnType.TINYINT:
+           return value.to_bytes(1)
+       elif column.type == ColumnType.TEXT:
+           return bytes(value, 'utf-8')
+       else:
+           raise Exception(f'cannot parse column type {column_type}')
+
+   def values_bytes(self) -> bytes:
+       body = bytearray()
+
+       for i, column in enumerate(self.columns):
+           body += self.value_bytes(column, self.values[i])
+
+       return bytes(body)
```

And we can create a function for serializing the entire record payload:

```python
# src/backend/record.py
...

   def values_bytes(self) -> bytes:
...
+    def to_bytes(self) -> bytes:
+        header_bytes = self.header_bytes()
+        values_bytes = self.values_bytes()
+
+        return header_bytes + values_bytes
```

Now we can add a test which deserializes, and then reserializes the record data. We'll use a simple example which has 3 columns, a TINYINT, an INTEGER and a VARCHAR of length 2:

```python
# test/backend/test_record.py

+   def test_record_read_write_payload(self):
+       # 4 byte header
+       # col 1 - tinyint, 17
+       # col 2 - integer, 1114129
+       # col 3 - string, 'OI'
+       payload = bytes.fromhex('0401041111001100114f49')
+       record = Record(payload, 0)
+       self.assertEqual(record.to_bytes(), payload)
```

This test takes an inbound payload, turns it into a record, and then verifies the output of the `to_bytes` function on the record matches the original payload.

We can follow on this work by adding a similar and simple `to_bytes` function to our `TableLeafCell` class:

```python
# src/backend/cell.py
...

+   def to_bytes(self):
+       payload = self.record.to_bytes()
+       payload_size_bytes = to_varint(len(payload))
+       row_id_bytes = to_varint(self.row_id)
+       return payload_size_bytes + row_id_bytes + payload

    def _debug(self):
```

And we can add a testing class to ensure this behavior works as well.

```python
# test/backend/test_cell.py

from unittest import TestCase

from src.backend.cell import TableLeafCell

class TestCell(TestCase):
    def test_table_leaf_cell(self):
        data = bytes([
            0x06, # payload size
            0x02, # row id
            0x03, 0x11, 0x01, 0x79, 0x6f, 0x02, # payload
        ])
        cell = TableLeafCell(data, 0)
        self.assertEqual(cell.payload_size, 6)
        self.assertEqual(cell.row_id, 2)
        self.assertEqual(cell.payload, data[2:])
        self.assertEqual(cell.cursor, 8)
        self.assertEqual(cell.to_bytes(), data)

if __name__ == '__main__':
    unittest.main()
```

Where the final assertion `self.assertEqual(cell.to_bytes(), data)` validates the behavior of turning the cell back into bytes.

# Serializing the Nodes

Building on the work done for the record values, we now can move onto the TableLeafNodes. As mentioned earlier, the data layout for these will be building from both the left and the right of the page, with the node header on the left and the cell payload on the right.

Getting this behavior to work requires us to build the cell data from the right to the left while keeping track of the pointers to the cell data so that we can add them to the right. It also requires us to effectively pad the space in the middle so that an appropriate amount of bytes is to be serialized to disk.

We also need to keep track of the page padding that's built into the SQLite engine so that we're compliant with the engine's configuration.

All of this requires a good amount of simple, yet precise arithmetic so we need to be careful with this step as it is probably mechanically the most complex step we've dealt with.

SQLite right pads the database pages - of which the setting when I created a db from scratch was 12 bytes of padding. Per the docs this information is included in the header:

```20   1   Bytes of unused "reserved" space at the end of each page. Usually 0.```
