<!doctype html><html lang=en><head><title>Home</title><meta charset=utf-8><meta content="utf-8" http-equiv=encoding><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=theme-color content="#000084"><link rel=icon href=https://abudlightlime.com/favicon.ico><link rel=canonical href=https://abudlightlime.com></head><body><nav class="navbar navbar-inverse navbar-fixed-top"><div class=navbar-inner><div class=container><button type=button class="btn btn-navbar" data-toggle=collapse data-target=.nav-collapse></button>
<a class=brand href=https://abudlightlime.com>Home</a><div class="nav-collapse collapse"><ul class=nav><li><a href=/about/><span>About</span></a></li><li><a href=/post/><span>All posts</span></a></li></ul></div></div></div></nav><div id=content class=container><div class="row-fluid navmargin"><div class=page-header><h1>Kubernetes Web Deployments, a love story - Tue, Mar 30, 2021</h1></div><p class=lead></p><h3 id=situation>Situation</h3><p>Let’s say that you’re a full stack developer that works on an application which is deployed top to bottom on kubernetes. One day your web deployment stalls and your team peers in to investigate - while Deployment B was scaling up some of its pods were unhealthy! Nothing to fear though, your team knows that kubernetes wouldn’t forward traffic to a pod if it doesn’t pass a healthcheck. Healthy pods = healthy deployment right?</p><p>It would seem that way until around 10 minutes later, when your product manager pops into chat frantically to say the site is down.</p><p>“The site is down?” You muse uncomfortably. If your service deployments go through some indirection like a sidecar proxy your first instinct could be to wonder if it could forward traffic to an unhealthy pod.</p><h3 id=root-cause-analysis>Root Cause Analysis</h3><p>So you, the full stack developer hop on a call with the big guns - infrastructure, platform, networking, etc. You share your theory that maybe a proxy sidecar was forwarding traffic to an unhealthy pod and you nearly get laughed out of the room. Then one of the platform engineers mentions a graph that she had been looking at and the room falls silent:</p><p><img src=/img/k8s-web-graph.png alt="Seeing an uptick in 404s">
<strong>Figure 1. You might not alert on 404s, because users tend to hit them all the time</strong></p><blockquote><p>&ldquo;Huh&mldr;we were getting about x 404s per second during this outage weren&rsquo;t we&mldr;&rdquo;</p></blockquote><p>&mldr;</p><blockquote><p>&ldquo;Well I wonder what the errors were&rdquo;</p></blockquote><p>And like that you jump to the logs.</p><pre><code>...
1964-03-15 8:23pm [SERVICE] paying citibike overage charges
1964-03-15 8:23pm [SERVICE] 404 no path named /static/a.minified.js &lt;---- heyo what's this??
1964-03-15 8:23pm [SERVICE] my cache way too i couldn't eat another bite
...
</code></pre><p>And in that moment you realize what had happened. For those who don’t know how modern web app deployments work, there’s generally a build step - where all the javascript gets compiled into a file (or set of files) with a unique name and is placed in some build directory. There’s an index.html file that is also built, which references those unique build files.</p><p>Because these build names will be unique between deployments (and therefore containers) a request for index.html to Deployment A - will trigger a request for a.minified.js by the browser. If that request gets routed to a pod in Deployment B the request will fail, because B will only have b.minified.js. In this way, kubernetes' safe rollout behavior became an ironically fatal quirk in your deployment.</p><p><img src=/img/k8s-web-ascii.png alt="The issue with two live deployments"></p><p><strong>Figure 2. Because is no guarantee requests stick to pods by client, a user could request static assests from the wrong deployment</strong></p><h3 id=what-you-should-know>What you should know</h3><p>Just because the stuck deployment caused an outage, it doesn’t mean that smooth deployments evade this issue. On most deployments there is some period where the new deployment is scaling up and the old one is scaling down and anytime you have two live deployments you can face this problem. The longer the scale up period; the longer the outage window - so if you’re considering deploying a web application to kubernetes, think long and hard about the availability requirements and deployment frequency.</p><h4><a href=https://abudlightlime.com>Back to Home</a></h4></div></div><footer class=container><hr class=soften><p><a href=https://gitlab.com/maxlefou/hugo.386>hugo.386 theme</a> |
&copy;
Brian Dillmann
<span id=thisyear>2020</span>
| brooklyn&rsquo;s noodle king
| Built on <a href=//gohugo.io target=_blank>Hugo</a></p><p class=text-center><a href=https://www.linkedin.com/in/brian-dillmann-0a997979/>Linkedin</a>
<a href=https://github.com/angles-n-daemons>GitHub</a></p></footer></body><link rel=stylesheet href=/css/bootstrap.css><link rel=stylesheet href=/css/bootstrap-responsive.css><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/customstyle.css><script src=/js/jquery.js></script><script src=/js/bootstrap-386.js></script><script src=/js/bootstrap-transition.js></script><script src=/js/bootstrap-alert.js></script><script src=/js/bootstrap-modal.js></script><script src=/js/bootstrap-dropdown.js></script><script src=/js/bootstrap-scrollspy.js></script><script src=/js/bootstrap-tab.js></script><script src=/js/bootstrap-tooltip.js></script><script src=/js/bootstrap-popover.js></script><script src=/js/bootstrap-button.js></script><script src=/js/bootstrap-collapse.js></script><script src=/js/bootstrap-carousel.js></script><script src=/js/bootstrap-typeahead.js></script><script src=/js/bootstrap-affix.js></script><script>_386={fastLoad:!1,onePass:!1,speedFactor:1};function ThisYear(){document.getElementById('thisyear').innerHTML=(new Date).getFullYear()}</script></html>